{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getting-started-ppo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianLarssonDTU/02456-Reinforcement-Learning-Project/blob/timed_training/getting_started_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhZaGWNrlFS"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import imageio\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone \n",
        "import time\n",
        "\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# Getting started with PPO and ProcGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LP1JU3I-d4"
      },
      "source": [
        "Here's a bit of code that should help you get started on your projects.\n",
        "\n",
        "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdpZ4lmFHtD8"
      },
      "source": [
        "!pip install procgen\n",
        "!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHgDWdZBqcf0"
      },
      "source": [
        "# Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9u2oj-xqfC_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJbEHviDqj6V"
      },
      "source": [
        "#check folders/files in drive\n",
        "!ls drive/'My Drive'/'02456-Deep-Learning-Project'\n",
        "DATA_PATH = 'drive/My Drive/02456-Deep-Learning-Project/Data/'\n",
        "MODEL_PATH = 'drive/My Drive/02456-Deep-Learning-Project/Models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHg_P6S2rfZh"
      },
      "source": [
        "def test_ability_to_create_file_on_drive():\n",
        "  #Check we can create a log file\n",
        "  columns = ['Step', 'Mean reward']\n",
        "  dummy_df = pd.DataFrame(np.random.randn(100, 2), columns=columns)\n",
        "  dummy_df\n",
        "  #Using , as seperator so I could open in google sheets and verify data\n",
        "  dummy_df.to_csv(DATA_PATH+'/dummy_test.csv', index=None, sep=',', mode='w')\n",
        "  \n",
        "  f = open(DATA_PATH+'/dummy_test.csv', \"a\")\n",
        "  f.write(\"\\n I can also append\")\n",
        "  f.close()\n",
        "\n",
        "#test_ability_to_create_file_on_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8yS7wPaoSMS"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGRFsNO2yNfz"
      },
      "source": [
        "# #Copied from https://lab-ml.com/labml_nn/rl/ppo/\n",
        "\n",
        "# def ClippedPPOLoss(log_pi: torch.Tensor, sampled_log_pi: torch.Tensor, advantage: torch.Tensor, clip: float) -> torch.Tensor:\n",
        "#   ratio = torch.exp(log_pi - sampled_log_pi)\n",
        "#   clipped_ratio = ratio.clamp(min=1.0 - clip, max=1.0 + clip)\n",
        "#   policy_reward = torch.min(ratio * advantage, clipped_ratio * advantage)\n",
        "#   #clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n",
        "#   return -policy_reward.mean()\n",
        "\n",
        "# def ClippedValueFunctionLoss(value: torch.Tensor, sampled_value: torch.Tensor, sampled_return: torch.Tensor, clip: float):\n",
        "#   clipped_value = sampled_value + (value - sampled_value).clamp(min=-clip, max=clip)\n",
        "#   vf_loss = torch.max((value - sampled_return) ** 2, (clipped_value - sampled_return) ** 2)\n",
        "#   return 0.5 * vf_loss.mean()\n",
        "\n",
        "\n",
        "def ClippedPPOLoss(advantage, log_pi, log_old_pi, eps):\n",
        "  ratio = torch.exp(log_pi - log_old_pi)\n",
        "  clipped_ratio = torch.clip(ratio, 1-eps, 1+eps)\n",
        "  clipped_reward = torch.min(ratio*advantage, clipped_ratio*advantage)\n",
        "  return clipped_reward.mean() \n",
        "\n",
        "def ValueFunctionLoss(new_value, old_value):\n",
        "  return ((new_value-old_value)**2).mean() \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMoOCfTC-DAs"
      },
      "source": [
        "def extract_data_from_csv(file_name):\n",
        "  df = pd.read_csv(DATA_PATH+file_name)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7eEfmqy-zr2"
      },
      "source": [
        "def create_index_table_from_txt_files():\n",
        "  all_txt_files = glob.glob(DATA_PATH +'*.txt')\n",
        "  final_df = pd.DataFrame()\n",
        "  \n",
        "  for file in all_txt_files:\n",
        "    df=pd.read_csv(file)\n",
        "    df = df.set_index('Parameter name')\n",
        "    df = df.transpose()\n",
        "    final_df = final_df.append(df)\n",
        "  return final_df\n",
        "\n",
        "#TODO: Is this the right result?\n",
        "def update_index_file_with_result(df):\n",
        "  df['Last Mean Reward'] = \"\"\n",
        "  for i in range(len(df)):\n",
        "    name = df['file_name'][i].strip()\n",
        "    #read csv file at DATA_PATH with current filname\n",
        "    f = open(DATA_PATH + name +'.csv', \"r\")\n",
        "    for last_line in f:\n",
        "        pass\n",
        "    f.close()\n",
        "\n",
        "    _, reward = last_line.split(\",\") \n",
        "    #add to table\n",
        "    df['Last Mean Reward'][i] = reward\n",
        "  return df\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "#Baseline Hyper Params\n",
        "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja6ruWVRYCpN"
      },
      "source": [
        "feature_dim= 512    # <- The only thing we chose ourself\n",
        "\n",
        "#Fixed values\n",
        "in_channels = 3 #RGB\n",
        "num_actions = 15  #Number of actions in the Procgen environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "source": [
        "#Hyperparameters from Notebook (Given by Nicklas)\n",
        "def set_hyperparameters_to_baseline_Nicklas():\n",
        "  global total_steps, num_envs, num_levels, num_steps, num_epochs, batch_size, eps, grad_eps, value_coef, entropy_coef, gamma, lmbda, lr, version\n",
        "  total_steps = 8e6\n",
        "  num_envs = 32\n",
        "  num_levels = 10\n",
        "  num_steps = 256\n",
        "  num_epochs = 3\n",
        "  batch_size = 512\n",
        "  eps = .2\n",
        "  grad_eps = .5\n",
        "  value_coef = .5\n",
        "  entropy_coef = .01\n",
        "  gamma = 0.99\n",
        "  lmbda = 0.95\n",
        "  lr= 5e-4\n",
        "  version='Baseline(Nicklas)'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEXcJ9RgWheF"
      },
      "source": [
        "#Hyperparameters inspired by PPO Paper ( without variable learning rate )\n",
        "def set_hyperparameters_to_baseline_PPO():\n",
        "  global total_steps, num_envs, num_levels, num_steps, num_epochs, batch_size, eps, grad_eps, value_coef, entropy_coef, gamma, lmbda, lr, version\n",
        "  total_steps = 8e6\n",
        "  num_envs = 32\n",
        "  num_levels = 10\n",
        "  num_steps = 128\n",
        "  num_epochs = 3\n",
        "  batch_size = 256\n",
        "  eps = .1\n",
        "  grad_eps = .5\n",
        "  value_coef = 1\n",
        "  entropy_coef = .01\n",
        "  gamma = 0.99\n",
        "  lmbda = 0.95\n",
        "  lr = 2.5e-4\n",
        "  version = 'Baseline(PPO)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bp8oS1wXipz"
      },
      "source": [
        "#Hyperparameters inspired by Procgen Paper (With 32 instead of 64 environments because of memory)\n",
        "def set_hyperparameters_to_baseline_Procgen():\n",
        "  global total_steps, num_envs, num_levels, num_steps, num_epochs, batch_size, eps, grad_eps, value_coef, entropy_coef, gamma, lmbda, lr , version\n",
        "  total_steps = 8e6\n",
        "  num_envs = 32\n",
        "  num_levels = 10\n",
        "  num_steps = 256\n",
        "  num_epochs = 3\n",
        "  batch_size = 512\n",
        "  eps = .2\n",
        "  grad_eps = .5\n",
        "  value_coef = .5\n",
        "  entropy_coef = .01\n",
        "  gamma = 0.999\n",
        "  lmbda = 0.95\n",
        "  lr= 5e-4\n",
        "  version = 'Baseline(Procgen)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzCWcHMolAAA"
      },
      "source": [
        "def set_hyperparameters(baseline='Procgen'):\n",
        "  implemented_baselines = {}\n",
        "  implemented_baselines['Procgen'] = set_hyperparameters_to_baseline_Procgen\n",
        "  implemented_baselines['PPO'] = set_hyperparameters_to_baseline_PPO\n",
        "\n",
        "  if baseline not in implemented_baselines.keys():\n",
        "    raise NotImplementedError(\"The implemented baselines are: {}\".format(implemented_baselines.keys()))\n",
        "  else:\n",
        "    implemented_baselines[baseline]()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRWy_T9JY4M"
      },
      "source": [
        "# Network definitions \n",
        "We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBV9xpKpEFa"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), \n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), \n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), \n",
        "        nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=feature_dim), \n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.apply(orthogonal_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    logits = self.policy(x)\n",
        "    value = self.value(x).squeeze(1)\n",
        "    #Creates a categorical distribution parameterized by either probs or logits\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "    #sample with dist.sample()\n",
        "    return dist, value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGQ8tNH_ooOR"
      },
      "source": [
        "# Training definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOmIp8fTojt2"
      },
      "source": [
        "\"\"\"\n",
        "  TODO: \n",
        "    1. Maybe update this to take hyperparams as input \n",
        "    2. Update to do data logging in files instead of as prints\n",
        "\"\"\"\n",
        "def train_network(print_output=False, \n",
        "                  file_name=None,\n",
        "                  total_steps = 8e6,\n",
        "                  num_envs = 32,\n",
        "                  num_levels = 10,\n",
        "                  num_steps = 256,\n",
        "                  num_epochs = 3,\n",
        "                  batch_size = 512,\n",
        "                  eps = .2,\n",
        "                  grad_eps = .5,\n",
        "                  value_coef = .5,\n",
        "                  entropy_coef = .01,\n",
        "                  lr=5e-4,\n",
        "                  opt_extra = 1e-5,\n",
        "                  gamma=0.99,\n",
        "                  lmbda = 0.95, \n",
        "                  version = '',\n",
        "                  optimizer = 'Adam'):    #does not change anything, just for logging purposes\n",
        "  \n",
        "  if file_name is None:\n",
        "    now = datetime.now(timezone('Europe/Copenhagen'))\n",
        "    file_name = version+'_Run_' + now.strftime(\"%d%b_%Hh%Mm%Ss\")\n",
        "    \n",
        "  hyper_params={}\n",
        "  \n",
        "  for key, val in reversed(list(locals().items())):\n",
        "    if key in ['print_output', 'hyper_params', 'now']:\n",
        "      continue\n",
        "    hyper_params[key] = val\n",
        "\n",
        "\n",
        "  \n",
        "  #create file\n",
        "  f = open(DATA_PATH+file_name+'.csv', \"w\")\n",
        "  f.write(\"Step, Mean reward\")\n",
        "  f.close()\n",
        "\n",
        "  #create txt file for hyper params\n",
        "  f = open(DATA_PATH+file_name+'.txt', \"w\")\n",
        "  f.write(\"Parameter name, Value\")\n",
        "  for key, val in hyper_params.items():  \n",
        "    f.write(\"\\n{}, {}\".format(key, val))\n",
        "  f.close()\n",
        "\n",
        "  # Define environment\n",
        "  # check the utils.py file for info on arguments\n",
        "  env = make_env(num_envs, num_levels=num_levels)\n",
        "  if print_output:\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "\n",
        "  encoder = Encoder(in_channels = in_channels, feature_dim = feature_dim)\n",
        "  policy = Policy(encoder = encoder, feature_dim = feature_dim, num_actions = num_actions)\n",
        "  policy.cuda()\n",
        "\n",
        "  # Define optimizer\n",
        "  # these are reasonable values but probably not optimal\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=lr, eps=opt_extra) #OBS: Remember to change dummy in input too for logging purposes\n",
        "\n",
        "  # Define temporary storage\n",
        "  # we use this to collect transitions during each iteration\n",
        "  storage = Storage(\n",
        "      env.observation_space.shape,\n",
        "      num_steps,\n",
        "      num_envs,\n",
        "      gamma = gamma,\n",
        "      lmbda = lmbda\n",
        "  )\n",
        "\n",
        "  # Run training\n",
        "  obs = env.reset()\n",
        "  step = 0\n",
        "  while step < total_steps:\n",
        "\n",
        "    # Use policy to collect data for num_steps steps\n",
        "    policy.eval()\n",
        "    for _ in range(num_steps):\n",
        "      # Use policy\n",
        "      action, log_prob, value = policy.act(obs)\n",
        "      \n",
        "      # Take step in environment\n",
        "      next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "      # Store data\n",
        "      storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "      \n",
        "      # Update current observation\n",
        "      obs = next_obs\n",
        "\n",
        "    # Add the last observation to collected data\n",
        "    _, _, value = policy.act(obs)\n",
        "    storage.store_last(obs, value)\n",
        "\n",
        "    # Compute return and advantage\n",
        "    storage.compute_return_advantage()\n",
        "\n",
        "    # Optimize policy\n",
        "    policy.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # Iterate over batches of transitions\n",
        "      generator = storage.get_generator(batch_size)\n",
        "      for batch in generator:\n",
        "        #Results from using old policy on environment\n",
        "        b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "        # Get current policy outputs\n",
        "        new_dist, new_value = policy(b_obs)\n",
        "        new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "        # Clipped policy objective\n",
        "        pi_loss = ClippedPPOLoss(advantage=b_advantage, \n",
        "                                 log_pi=new_log_prob, \n",
        "                                 log_old_pi=b_log_prob, \n",
        "                                 eps=eps)\n",
        "\n",
        "\n",
        "        # # Clipped value function objective\n",
        "        # #Assume value_loss = ClippedValueFunctionLoss \n",
        "        value_loss= ValueFunctionLoss(new_value=new_value, \n",
        "                                      old_value= b_value)\n",
        "\n",
        "        # Entropy loss\n",
        "        entropy_loss = new_dist.entropy().mean()\n",
        "\n",
        "        # Backpropagate losses\n",
        "        loss = -(pi_loss - value_coef * value_loss + entropy_coef*entropy_loss)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "        # Update policy\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Update stats\n",
        "    step += num_envs * num_steps\n",
        "    if print_output:\n",
        "      print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "    f = open(DATA_PATH+file_name+'.csv', \"a\")\n",
        "    f.write(\"\\n{}, {}\".format(step, storage.get_reward()))\n",
        "    f.close()\n",
        "\n",
        "  if print_output:\n",
        "    print('Completed training!')\n",
        "  torch.save(policy.state_dict, MODEL_PATH + file_name+'.pt')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CNPOqnFpaNC"
      },
      "source": [
        "# Timed Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ORjTQ-pdCi"
      },
      "source": [
        "def timed_training(print_output=False, \n",
        "                  file_name=None,\n",
        "                  total_steps = 8e6,\n",
        "                  num_envs = 32,\n",
        "                  num_levels = 10,\n",
        "                  num_steps = 256,\n",
        "                  num_epochs = 3,\n",
        "                  batch_size = 512,\n",
        "                  eps = .2,\n",
        "                  grad_eps = .5,\n",
        "                  value_coef = .5,\n",
        "                  entropy_coef = .01,\n",
        "                  lr=5e-4,\n",
        "                  opt_extra = 1e-5,\n",
        "                  gamma=0.99,\n",
        "                  lmbda = 0.95, \n",
        "                  version = '',\n",
        "                  optimizer = 'Adam',\n",
        "                  time_limit_hours = 0,\n",
        "                  time_limit_minutes = 30,\n",
        "                  time_limit_seconds = 0):   #They will be added together\n",
        "  \n",
        "  if file_name is None:\n",
        "    now = datetime.now(timezone('Europe/Copenhagen'))\n",
        "    file_name = version+'_Run_' + now.strftime(\"%d%b_%Hh%Mm%Ss\")\n",
        "    \n",
        "  hyper_params={}\n",
        "  \n",
        "  for key, val in reversed(list(locals().items())):\n",
        "    if key in ['print_output', 'hyper_params', 'now', 'time_limit_hours', 'time_limit_seconds', 'time_limit_minutes']:\n",
        "      continue\n",
        "    hyper_params[key] = val\n",
        "\n",
        "\n",
        "  \n",
        "  #create file\n",
        "  f = open(DATA_PATH+file_name+'.csv', \"w\")\n",
        "  f.write(\"Step, Mean reward\")\n",
        "  f.close()\n",
        "\n",
        "  #create txt file for hyper params\n",
        "  f = open(DATA_PATH+file_name+'.txt', \"w\")\n",
        "  f.write(\"Parameter name, Value\")\n",
        "  for key, val in hyper_params.items():  \n",
        "    f.write(\"\\n{}, {}\".format(key, val))\n",
        "  f.close()\n",
        "\n",
        "  #start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Define environment\n",
        "  # check the utils.py file for info on arguments\n",
        "  env = make_env(num_envs, num_levels=num_levels)\n",
        "  if print_output:\n",
        "    print('Observation space:', env.observation_space)\n",
        "    print('Action space:', env.action_space.n)\n",
        "\n",
        "  encoder = Encoder(in_channels = in_channels, feature_dim = feature_dim)\n",
        "  policy = Policy(encoder = encoder, feature_dim = feature_dim, num_actions = num_actions)\n",
        "  policy.cuda()\n",
        "\n",
        "  # Define optimizer\n",
        "  # these are reasonable values but probably not optimal\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=lr, eps=opt_extra) #OBS: Remember to change dummy in input too for logging purposes\n",
        "\n",
        "  # Define temporary storage\n",
        "  # we use this to collect transitions during each iteration\n",
        "  storage = Storage(\n",
        "      env.observation_space.shape,\n",
        "      num_steps,\n",
        "      num_envs,\n",
        "      gamma = gamma,\n",
        "      lmbda = lmbda\n",
        "  )\n",
        "\n",
        "  # Run training\n",
        "  obs = env.reset()\n",
        "  step = 0\n",
        "\n",
        "  #calculate time limit in seconds\n",
        "  time_limit = 60*60*time_limit_hours+60*time_limit_minutes + time_limit_seconds\n",
        "  f = open(DATA_PATH+file_name+'.txt', \"a\")\n",
        "  f.write(\"\\n Time limit, {:.0f}:{:.0f}:{:.0f}\".format(time_limit_hours, time_limit_minutes, time_limit_seconds))\n",
        "  f.close()\n",
        "\n",
        "\n",
        "  while step < total_steps:\n",
        "\n",
        "    #Test time_limit\n",
        "    now = time.time()\n",
        "    if now-start_time > time_limit:\n",
        "      end_time = now\n",
        "      #Add to log file\n",
        "      f = open(DATA_PATH+file_name+'.txt', \"a\")\n",
        "      f.write(\"\\n Time spent (in seconds), {:.2f}\".format(end_time-start_time))\n",
        "      f.write(\"\\n Steps taken, {}\".format(step))\n",
        "      f.write(\"\\n Done, False\")\n",
        "      f.close()\n",
        "      torch.save(policy.state_dict, MODEL_PATH + file_name+'.pt')\n",
        "      return\n",
        "\n",
        "    # Use policy to collect data for num_steps steps\n",
        "    policy.eval()\n",
        "    for _ in range(num_steps):\n",
        "      # Use policy\n",
        "      action, log_prob, value = policy.act(obs)\n",
        "      \n",
        "      # Take step in environment\n",
        "      next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "      # Store data\n",
        "      storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "      \n",
        "      # Update current observation\n",
        "      obs = next_obs\n",
        "\n",
        "    # Add the last observation to collected data\n",
        "    _, _, value = policy.act(obs)\n",
        "    storage.store_last(obs, value)\n",
        "\n",
        "    # Compute return and advantage\n",
        "    storage.compute_return_advantage()\n",
        "\n",
        "    # Optimize policy\n",
        "    policy.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # Iterate over batches of transitions\n",
        "      generator = storage.get_generator(batch_size)\n",
        "      for batch in generator:\n",
        "        #Results from using old policy on environment\n",
        "        b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "        # Get current policy outputs\n",
        "        new_dist, new_value = policy(b_obs)\n",
        "        new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "        # Clipped policy objective\n",
        "        pi_loss = ClippedPPOLoss(advantage=b_advantage, \n",
        "                                 log_pi=new_log_prob, \n",
        "                                 log_old_pi=b_log_prob, \n",
        "                                 eps=eps)\n",
        "\n",
        "\n",
        "        # # Clipped value function objective\n",
        "        # #Assume value_loss = ClippedValueFunctionLoss \n",
        "        value_loss= ValueFunctionLoss(new_value=new_value, \n",
        "                                      old_value= b_value)\n",
        "\n",
        "        # Entropy loss\n",
        "        entropy_loss = new_dist.entropy().mean()\n",
        "\n",
        "        # Backpropagate losses\n",
        "        loss = -(pi_loss - value_coef * value_loss + entropy_coef*entropy_loss)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "        # Update policy\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Update stats\n",
        "    step += num_envs * num_steps\n",
        "    if print_output:\n",
        "      print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "    f = open(DATA_PATH+file_name+'.csv', \"a\")\n",
        "    f.write(\"\\n{}, {}\".format(step, storage.get_reward()))\n",
        "    f.close()\n",
        "\n",
        "  if print_output:\n",
        "    print('Completed training!')\n",
        "\n",
        "  #Add to log file\n",
        "  end_time = time.time()\n",
        "  f = open(DATA_PATH+file_name+'.txt', \"a\")\n",
        "  f.write(\"\\n Time spent (in seconds), {:.2f}\".format(end_time-start_time))\n",
        "  f.write(\"\\n Steps taken, {}\".format(step))\n",
        "  f.write(\"\\n Done, True\")\n",
        "  f.close()\n",
        "  torch.save(policy.state_dict, MODEL_PATH + file_name+'.pt')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9QZNXXfN7Az"
      },
      "source": [
        "#Do Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWR_bVCg6QJc"
      },
      "source": [
        "timed_training(total_steps=4e4, version='test', print_output= True, time_limit_minutes=0, time_limit_seconds=30)\n",
        "\n",
        "# set_hyperparameters(baseline='Procgen')\n",
        "# train_network(total_steps = total_steps,\n",
        "#               num_envs = num_envs,\n",
        "#               num_levels = num_levels,\n",
        "#               num_steps = num_steps,\n",
        "#               num_epochs = num_epochs,\n",
        "#               batch_size = batch_size,\n",
        "#               eps = eps,\n",
        "#               grad_eps = grad_eps,\n",
        "#               value_coef = value_coef,\n",
        "#               entropy_coef = entropy_coef,\n",
        "#               gamma = gamma,\n",
        "#               lmbda = lmbda,\n",
        "#               lr= lr,\n",
        "#               version = version,\n",
        "#               print_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAkAKU8wsMQN"
      },
      "source": [
        "df = create_index_table_from_txt_files()\n",
        "update_index_file_with_result(df)\n",
        "# df[[\"file_name\", \"Last Mean Reward\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZrWuVGLTu-"
      },
      "source": [
        "# Post training processing \n",
        "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zecOCkd7Jzt"
      },
      "source": [
        "def policy_evaluation(video_name='vid', print_output=False):\n",
        "  # Make evaluation environment\n",
        "  eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels)\n",
        "  obs = eval_env.reset()\n",
        "\n",
        "  frames = []\n",
        "  total_reward = []\n",
        "\n",
        "  # Evaluate policy\n",
        "  policy.eval()\n",
        "  for _ in range(512):\n",
        "\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "    # Render environment and store\n",
        "    frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "    frames.append(frame)\n",
        "\n",
        "  # Calculate average return\n",
        "  total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "  if print_output:\n",
        "    print('Average return:', total_reward)\n",
        "\n",
        "  # Save frames as video\n",
        "  frames = torch.stack(frames)\n",
        "  imageio.mimsave(video_name+'.mp4', frames, fps=25)\n",
        "  \n",
        "  return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlCHEjD-FXof"
      },
      "source": [
        "# Grid search ?\n"
      ]
    }
  ]
}