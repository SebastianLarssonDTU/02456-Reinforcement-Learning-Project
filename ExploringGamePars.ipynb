{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarpilotProject.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnPCpfsOA/kqrQtNh33WwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianLarssonDTU/02456-Reinforcement-Learning-Project/blob/restructure_code/ExploringGamePars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jlWJKXBo0m"
      },
      "source": [
        "# INIT : Procgen, Drive, Git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mmSs5ItNLLI"
      },
      "source": [
        "!pip install procgen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RgF6myEA6WL"
      },
      "source": [
        "#Clone git\n",
        "!git clone -b restructure_code https://github.com/SebastianLarssonDTU/02456-Reinforcement-Learning-Project.git \"my_project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jsbleFHMz7"
      },
      "source": [
        "#update git\n",
        "%cd /content/my_project\n",
        "! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abnnrXLuB6Ry"
      },
      "source": [
        "import datatools as tools\n",
        "from datatools import DATA_PATH, MODEL_PATH\n",
        "#Mount drive\n",
        "tools.mount_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBb1c_XBBsNl"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XmB8QZ7BQIW"
      },
      "source": [
        "#Import all custom files\n",
        "import baseline\n",
        "import datatools as tools\n",
        "import hyperparameters as h\n",
        "import model\n",
        "import my_util\n",
        "import policy\n",
        "import ppo\n",
        "import utils\n",
        "\n",
        "#other imports\n",
        "import torch\n",
        "\n",
        "#import specific methods\n",
        "from baseline import set_hyperparameters\n",
        "from ppo import PPO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4kkq7b3LA3"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxiGBAGACpMJ"
      },
      "source": [
        "#The hyperparameters for the model is set in the hyperparameter module.\n",
        "\n",
        "#choose a baseline\n",
        "set_hyperparameters(baseline='Procgen')\n",
        "\n",
        "#Or set manually\n",
        "h.version=\"Test\"\n",
        "h.time_limit_hours=0\n",
        "h.time_limit_minutes=10\n",
        "\n",
        "#create model using current configuration in h (creating a model will also create log files on drive)\n",
        "# model = PPO(print_output=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14zdzxRz3NPh"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY1tH-tsCrjl"
      },
      "source": [
        "# policy = model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGtJEwdwR0uF"
      },
      "source": [
        "tools.create_index_table_from_txt_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WeoqoYOS4_T"
      },
      "source": [
        "from utils import make_env\n",
        "env = make_env(2, num_levels=10)\n",
        "obs = env.reset()\n",
        "\n",
        "h.num_env = 2\n",
        "model = PPO(print_output=True)\n",
        "policy = model.policy\n",
        "action, log_prob, value = policy.act(obs)\n",
        "next_obs, reward, done, info = env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRgGsyljL1Ca"
      },
      "source": [
        "policy = model.policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBlwnr2InUj"
      },
      "source": [
        "obs = env.reset()\n",
        "\n",
        "\n",
        "for step in range(128):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    #print\n",
        "    # print(\"Step {}\".format(step))\n",
        "    if True in done:\n",
        "      print(\"[{}] Environments {} are done\".format(step, [index for index in range(len(done)) if done[index] == True]))\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0:\n",
        "        print(\"[{}] Environment {} got reward of {}\".format(step, i, reward[i]))\n",
        "    \n",
        "\n",
        "    # Update current observation\n",
        "    obs = next_obs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fosaLZ9_JAbj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_policy(model, \n",
        "                    nr_of_levels,\n",
        "                    print_output=True):\n",
        "  \"\"\"\n",
        "  TODO: Add Video generation\n",
        "  \"\"\"\n",
        "  policy = model.policy\n",
        "\n",
        "  #pick levels we did not train on. \n",
        "  eval_env = utils.make_env(model.num_envs, start_level=model.num_levels, num_levels=nr_of_levels)\n",
        "  obs = eval_env.reset()\n",
        "\n",
        "  #book-keeping\n",
        "  completed_envs= []\n",
        "  counter_compl_envs = np.zeros(model.num_envs)\n",
        "  episode_rewards = np.zeros(model.num_envs)  #current episode rewards\n",
        "  rewards = {}\n",
        "  for i in range(model.num_envs):\n",
        "    rewards[i] = []\n",
        "  step_counter = 0\n",
        "\n",
        "  policy.eval()\n",
        "  while True:\n",
        "\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    \n",
        "    #if any reward, update envs still not done\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0 and i not in completed_envs:\n",
        "        episode_rewards[i] += reward[i]\n",
        "    \n",
        "    # If new environment done, complete it\n",
        "    for i in [index for index in range(len(done)) if done[index] == True]:\n",
        "      if i not in completed_envs:\n",
        "        counter_compl_envs[i] += 1\n",
        "        if print_output:\n",
        "          print(\"Environment {:2d} completed its {:4d}th level at timestep {:6d} with a reward of {:10f}\".format(i, int(counter_compl_envs[i]), step_counter, episode_rewards[i]))\n",
        "        rewards[i].append(episode_rewards[i])\n",
        "        episode_rewards[i] = 0\n",
        "        if counter_compl_envs[i] == nr_of_levels:\n",
        "          completed_envs.append(i)  \n",
        "        \n",
        " \n",
        "\n",
        "    # If all environments are done, break\n",
        "    if len(completed_envs) == model.num_envs:\n",
        "      break\n",
        "    step_counter +=1\n",
        "  # end while\n",
        "  \n",
        "  # Calculate average return\n",
        "  total_reward = []\n",
        "  for key, value in rewards.items():\n",
        "    total_reward.append(sum(value))\n",
        "  total_reward = np.mean(total_reward)/nr_of_levels\n",
        "\n",
        "  if print_output:\n",
        "    print('Average return:', total_reward)\n",
        "\n",
        "  return total_reward, episode_rewards\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM1n8FMIO5ms"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48h4whJwVxfg"
      },
      "source": [
        "start = time.time()\n",
        "total_reward, episode_rewards = evaluate_policy(model, 1)\n",
        "print(time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxyY7edYbqAI"
      },
      "source": [
        "for _ in range(4):\n",
        "  total_reward, episode_rewards = evaluate_policy(model, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ik_IL1eWH4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}