{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarpilotProject.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbu4yvly2ux9OGqLg54Vwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianLarssonDTU/02456-Reinforcement-Learning-Project/blob/restructure_code/ExploringGamePars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jlWJKXBo0m"
      },
      "source": [
        "# INIT : Procgen, Drive, Git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mmSs5ItNLLI"
      },
      "source": [
        "!pip install procgen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RgF6myEA6WL"
      },
      "source": [
        "#Clone git\n",
        "!git clone -b restructure_code https://github.com/SebastianLarssonDTU/02456-Reinforcement-Learning-Project.git \"my_project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jsbleFHMz7"
      },
      "source": [
        "#update git\n",
        "%cd /content/my_project\n",
        "! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abnnrXLuB6Ry"
      },
      "source": [
        "import datatools as tools\n",
        "from datatools import DATA_PATH, MODEL_PATH\n",
        "#Mount drive\n",
        "tools.mount_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBb1c_XBBsNl"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XmB8QZ7BQIW"
      },
      "source": [
        "#Import all custom files\n",
        "import baseline\n",
        "import datatools as tools\n",
        "import hyperparameters as h\n",
        "import model\n",
        "import my_util\n",
        "import policy\n",
        "import ppo\n",
        "import utils\n",
        "\n",
        "#other imports\n",
        "import torch\n",
        "\n",
        "#import specific methods\n",
        "from baseline import set_hyperparameters\n",
        "from ppo import PPO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4kkq7b3LA3"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxiGBAGACpMJ"
      },
      "source": [
        "#The hyperparameters for the model is set in the hyperparameter module.\n",
        "\n",
        "#choose a baseline\n",
        "set_hyperparameters(baseline='Procgen')\n",
        "\n",
        "#Or set manually\n",
        "h.version=\"Test\"\n",
        "h.time_limit_hours=0\n",
        "h.time_limit_minutes=10\n",
        "\n",
        "#create model using current configuration in h (creating a model will also create log files on drive)\n",
        "# model = PPO(print_output=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14zdzxRz3NPh"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY1tH-tsCrjl"
      },
      "source": [
        "# policy = model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGtJEwdwR0uF"
      },
      "source": [
        "tools.create_index_table_from_txt_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WeoqoYOS4_T"
      },
      "source": [
        "from utils import make_env\n",
        "env = make_env(32, num_levels=10)\n",
        "obs = env.reset()\n",
        "\n",
        "model = PPO(print_output=True)\n",
        "policy = model.policy\n",
        "action, log_prob, value = policy.act(obs)\n",
        "next_obs, reward, done, info = env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRgGsyljL1Ca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBlwnr2InUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088a834e-fe13-4154-9c4e-dd9aabc77e08"
      },
      "source": [
        "obs = env.reset()\n",
        "\n",
        "\n",
        "for step in range(128):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    #print\n",
        "    # print(\"Step {}\".format(step))\n",
        "    if True in done:\n",
        "      print(\"[{}] Environments {} are done\".format(step, [index for index in range(len(done)) if done[index] == True]))\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0:\n",
        "        print(\"[{}] Environment {} got reward of {}\".format(step, i, reward[i]))\n",
        "    \n",
        "\n",
        "    # Update current observation\n",
        "    obs = next_obs\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: manual reset ignored\n",
            "[0] Environments [31] are done\n",
            "[0] Environment 3 got reward of 1.4214231967926025\n",
            "[0] Environment 9 got reward of 1.4214231967926025\n",
            "[0] Environment 17 got reward of 1.4214231967926025\n",
            "[1] Environment 28 got reward of 1.4234546422958374\n",
            "[2] Environment 19 got reward of 1.424985408782959\n",
            "[2] Environment 28 got reward of 1.424985408782959\n",
            "[5] Environment 8 got reward of 1.4291764497756958\n",
            "[5] Environment 17 got reward of 1.4291764497756958\n",
            "[6] Environments [24] are done\n",
            "[6] Environment 24 got reward of 1.4302071332931519\n",
            "[7] Environment 3 got reward of 1.430940866470337\n",
            "[8] Environment 29 got reward of 1.4315505027770996\n",
            "[8] Environment 30 got reward of 1.4315505027770996\n",
            "[9] Environments [16] are done\n",
            "[10] Environment 9 got reward of 1.4324977397918701\n",
            "[11] Environment 19 got reward of 1.4324533939361572\n",
            "[12] Environments [29] are done\n",
            "[13] Environments [1] are done\n",
            "[14] Environment 28 got reward of 1.4321144819259644\n",
            "[16] Environment 28 got reward of 1.4303642511367798\n",
            "[17] Environment 3 got reward of 1.4284727573394775\n",
            "[18] Environments [30] are done\n",
            "[18] Environment 9 got reward of 1.4259698390960693\n",
            "[18] Environment 15 got reward of 1.4259698390960693\n",
            "[21] Environments [21] are done\n",
            "[24] Environments [13] are done\n",
            "[24] Environment 3 got reward of 1.413061499595642\n",
            "[25] Environment 15 got reward of 1.4102568626403809\n",
            "[27] Environment 5 got reward of 1.4050158262252808\n",
            "[28] Environment 9 got reward of 1.4017540216445923\n",
            "[29] Environment 3 got reward of 1.3969693183898926\n",
            "[29] Environment 15 got reward of 1.3969693183898926\n",
            "[30] Environments [28] are done\n",
            "[31] Environments [20] are done\n",
            "[31] Environment 25 got reward of 1.3892370462417603\n",
            "[34] Environments [19] are done\n",
            "[36] Environment 18 got reward of 1.3756061792373657\n",
            "[36] Environment 25 got reward of 1.3756061792373657\n",
            "[37] Environments [17] are done\n",
            "[38] Environment 15 got reward of 1.3700670003890991\n",
            "[38] Environment 24 got reward of 1.3700670003890991\n",
            "[39] Environments [7, 8] are done\n",
            "[40] Environments [3, 10] are done\n",
            "[43] Environments [26] are done\n",
            "[43] Environment 9 got reward of 1.3611024618148804\n",
            "[43] Environment 26 got reward of 1.3611024618148804\n",
            "[44] Environments [22] are done\n",
            "[45] Environments [23] are done\n",
            "[47] Environment 28 got reward of 1.355597972869873\n",
            "[49] Environment 24 got reward of 1.352995753288269\n",
            "[50] Environment 2 got reward of 1.3516316413879395\n",
            "[52] Environment 31 got reward of 1.3491106033325195\n",
            "[53] Environment 9 got reward of 1.346511960029602\n",
            "[53] Environment 24 got reward of 1.346511960029602\n",
            "[54] Environment 10 got reward of 1.3439948558807373\n",
            "[57] Environment 30 got reward of 1.3368761539459229\n",
            "[57] Environment 31 got reward of 1.3368761539459229\n",
            "[58] Environments [20, 25] are done\n",
            "[59] Environments [5] are done\n",
            "[61] Environments [0, 12] are done\n",
            "[62] Environment 2 got reward of 1.3267364501953125\n",
            "[62] Environment 29 got reward of 1.3267364501953125\n",
            "[64] Environment 15 got reward of 1.3223674297332764\n",
            "[66] Environments [18] are done\n",
            "[70] Environments [6] are done\n",
            "[71] Environment 19 got reward of 1.3079423904418945\n",
            "[71] Environment 31 got reward of 1.3079423904418945\n",
            "[72] Environments [4] are done\n",
            "[74] Environment 30 got reward of 1.3020087480545044\n",
            "[75] Environments [9, 15] are done\n",
            "[75] Environment 28 got reward of 1.299904465675354\n",
            "[76] Environments [31] are done\n",
            "[76] Environment 19 got reward of 1.2997570037841797\n",
            "[77] Environments [23] are done\n",
            "[81] Environment 2 got reward of 1.3016587495803833\n",
            "[82] Environment 27 got reward of 1.3018075227737427\n",
            "[83] Environments [2, 17] are done\n",
            "[83] Environment 11 got reward of 1.3019541501998901\n",
            "[86] Environments [16] are done\n",
            "[90] Environments [3] are done\n",
            "[90] Environment 19 got reward of 1.3065646886825562\n",
            "[91] Environment 5 got reward of 1.3069523572921753\n",
            "[92] Environments [5, 19] are done\n",
            "[92] Environment 0 got reward of 1.3070752620697021\n",
            "[92] Environment 11 got reward of 1.3070752620697021\n",
            "[92] Environment 12 got reward of 1.3070752620697021\n",
            "[93] Environments [14] are done\n",
            "[94] Environments [21] are done\n",
            "[95] Environments [8, 11, 27] are done\n",
            "[95] Environment 27 got reward of 1.3090592622756958\n",
            "[96] Environments [24] are done\n",
            "[97] Environment 29 got reward of 1.3111337423324585\n",
            "[98] Environment 12 got reward of 1.311982274055481\n",
            "[100] Environments [30] are done\n",
            "[100] Environment 12 got reward of 1.3133119344711304\n",
            "[105] Environments [4] are done\n",
            "[105] Environment 12 got reward of 1.315922737121582\n",
            "[107] Environment 0 got reward of 1.3160102367401123\n",
            "[109] Environments [29] are done\n",
            "[111] Environments [22] are done\n",
            "[114] Environment 0 got reward of 1.3168920278549194\n",
            "[117] Environments [13] are done\n",
            "[118] Environments [12, 14] are done\n",
            "[118] Environment 0 got reward of 1.3164323568344116\n",
            "[119] Environment 21 got reward of 1.3167946338653564\n",
            "[123] Environment 8 got reward of 1.318440556526184\n",
            "[125] Environments [7, 23] are done\n",
            "[125] Environment 21 got reward of 1.3191322088241577\n",
            "[127] Environment 1 got reward of 1.3196780681610107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj4uor27N_oo",
        "outputId": "b3a7835a-3679-4dee-c178-928da08a3874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ppo.PPO at 0x7f401a194278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fosaLZ9_JAbj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_policy(model, \n",
        "                    nr_of_levels,\n",
        "                    print_output=True):\n",
        "  \"\"\"\n",
        "  TODO: Add Video generation\n",
        "  \"\"\"\n",
        "  policy = model.policy\n",
        "\n",
        "  #pick levels we did not train on. \n",
        "  eval_env = utils.make_env(model.num_envs, start_level=model.num_levels, num_levels=nr_of_levels)\n",
        "  obs = eval_env.reset()\n",
        "\n",
        "  #book-keeping\n",
        "  completed_envs= []\n",
        "  episode_rewards = np.zeros(model.num_envs)\n",
        "  step_counter = 0\n",
        "\n",
        "  policy.eval()\n",
        "  while True:\n",
        "\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    \n",
        "    #if any reward, update envs still not done\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0 and i not in completed_envs:\n",
        "        episode_rewards[i] += reward[i]\n",
        "    \n",
        "    # If new environment done, complete it\n",
        "    for i in [index for index in range(len(done)) if done[index] == True]:\n",
        "      if i not in completed_envs:\n",
        "        if print_output:\n",
        "          print(\"Environment {:2d} completed at timestep {:6d} with a reward of {:10f}\".format(i, step_counter, episode_rewards[i]))\n",
        "        completed_envs.append(i)   \n",
        "\n",
        "    # If all environments are done, break\n",
        "    if len(completed_envs) == model.num_envs:\n",
        "      break\n",
        "    step_counter +=1\n",
        "  # end while\n",
        "  \n",
        "  # Calculate average return\n",
        "  total_reward = episode_rewards.mean()\n",
        "\n",
        "  if print_output:\n",
        "    print('Average return:', total_reward)\n",
        "\n",
        "  return total_reward\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM1n8FMIO5ms",
        "outputId": "c4912bff-911e-4954-b7af-4fadc3cfee2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate_policy(model, 10)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enviroment 18 completed at timestep     49 with a reward of   0.000000\n",
            "Enviroment 25 completed at timestep     50 with a reward of   0.000000\n",
            "Enviroment 30 completed at timestep     50 with a reward of   0.000000\n",
            "Enviroment  6 completed at timestep     51 with a reward of   0.000000\n",
            "Enviroment 29 completed at timestep     51 with a reward of   0.000000\n",
            "Enviroment  1 completed at timestep     54 with a reward of   0.000000\n",
            "Enviroment  5 completed at timestep     62 with a reward of   1.411497\n",
            "Enviroment 12 completed at timestep     65 with a reward of   3.126455\n",
            "Enviroment 15 completed at timestep     65 with a reward of   2.394083\n",
            "Enviroment 13 completed at timestep     68 with a reward of   1.112146\n",
            "Enviroment 28 completed at timestep     68 with a reward of   2.431997\n",
            "Enviroment 14 completed at timestep     70 with a reward of   3.570373\n",
            "Enviroment  4 completed at timestep     71 with a reward of   3.550746\n",
            "Enviroment 21 completed at timestep     73 with a reward of   1.112146\n",
            "Enviroment 27 completed at timestep     79 with a reward of   3.899243\n",
            "Enviroment 17 completed at timestep     81 with a reward of   0.000000\n",
            "Enviroment 11 completed at timestep     84 with a reward of  17.225828\n",
            "Enviroment 23 completed at timestep     87 with a reward of   0.000000\n",
            "Enviroment 24 completed at timestep     88 with a reward of   2.283421\n",
            "Enviroment 20 completed at timestep     89 with a reward of  13.681280\n",
            "Enviroment 31 completed at timestep     89 with a reward of   1.485199\n",
            "Enviroment  2 completed at timestep     90 with a reward of  20.006156\n",
            "Enviroment  3 completed at timestep     90 with a reward of  10.395222\n",
            "Enviroment  0 completed at timestep     91 with a reward of   3.331611\n",
            "Enviroment 19 completed at timestep     91 with a reward of  12.135684\n",
            "Enviroment 22 completed at timestep    119 with a reward of   7.354867\n",
            "Enviroment 16 completed at timestep    124 with a reward of  17.983175\n",
            "Enviroment 10 completed at timestep    145 with a reward of  13.578997\n",
            "Enviroment  9 completed at timestep    154 with a reward of   0.972435\n",
            "Enviroment  7 completed at timestep    158 with a reward of   2.263318\n",
            "Enviroment 26 completed at timestep    164 with a reward of  13.981824\n",
            "Enviroment  8 completed at timestep    191 with a reward of   5.799857\n",
            "Average return: 5.158986268565059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.158986268565059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7EF_ApcQ-kL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}