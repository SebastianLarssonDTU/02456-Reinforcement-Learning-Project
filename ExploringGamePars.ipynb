{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarpilotProject.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwjtg4vWm/xGdzmQWhKVXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianLarssonDTU/02456-Reinforcement-Learning-Project/blob/restructure_code/ExploringGamePars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6jlWJKXBo0m"
      },
      "source": [
        "# INIT : Procgen, Drive, Git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mmSs5ItNLLI"
      },
      "source": [
        "!pip install procgen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RgF6myEA6WL"
      },
      "source": [
        "#Clone git\n",
        "!git clone -b restructure_code https://github.com/SebastianLarssonDTU/02456-Reinforcement-Learning-Project.git \"my_project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jsbleFHMz7"
      },
      "source": [
        "#update git\n",
        "%cd /content/my_project\n",
        "! git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abnnrXLuB6Ry"
      },
      "source": [
        "import datatools as tools\n",
        "from datatools import DATA_PATH, MODEL_PATH\n",
        "#Mount drive\n",
        "tools.mount_drive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBb1c_XBBsNl"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XmB8QZ7BQIW"
      },
      "source": [
        "#Import all custom files\n",
        "import baseline\n",
        "import datatools as tools\n",
        "import hyperparameters as h\n",
        "import model\n",
        "import my_util\n",
        "import policy\n",
        "import ppo\n",
        "import utils\n",
        "\n",
        "#other imports\n",
        "import torch\n",
        "\n",
        "#import specific methods\n",
        "from baseline import set_hyperparameters\n",
        "from ppo import PPO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4kkq7b3LA3"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxiGBAGACpMJ"
      },
      "source": [
        "#The hyperparameters for the model is set in the hyperparameter module.\n",
        "\n",
        "#choose a baseline\n",
        "set_hyperparameters(baseline='Procgen')\n",
        "\n",
        "#Or set manually\n",
        "h.version=\"Test\"\n",
        "h.time_limit_hours=0\n",
        "h.time_limit_minutes=10\n",
        "\n",
        "#create model using current configuration in h (creating a model will also create log files on drive)\n",
        "# model = PPO(print_output=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14zdzxRz3NPh"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY1tH-tsCrjl"
      },
      "source": [
        "# policy = model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGtJEwdwR0uF"
      },
      "source": [
        "tools.create_index_table_from_txt_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WeoqoYOS4_T"
      },
      "source": [
        "from utils import make_env\n",
        "env = make_env(32, num_levels=10)\n",
        "obs = env.reset()\n",
        "\n",
        "model = PPO(print_output=True)\n",
        "policy = model.policy\n",
        "action, log_prob, value = policy.act(obs)\n",
        "next_obs, reward, done, info = env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRgGsyljL1Ca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oBlwnr2InUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088a834e-fe13-4154-9c4e-dd9aabc77e08"
      },
      "source": [
        "obs = env.reset()\n",
        "\n",
        "\n",
        "for step in range(128):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    #print\n",
        "    # print(\"Step {}\".format(step))\n",
        "    if True in done:\n",
        "      print(\"[{}] Environments {} are done\".format(step, [index for index in range(len(done)) if done[index] == True]))\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0:\n",
        "        print(\"[{}] Environment {} got reward of {}\".format(step, i, reward[i]))\n",
        "    \n",
        "\n",
        "    # Update current observation\n",
        "    obs = next_obs\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: manual reset ignored\n",
            "[0] Environments [31] are done\n",
            "[0] Environment 3 got reward of 1.4214231967926025\n",
            "[0] Environment 9 got reward of 1.4214231967926025\n",
            "[0] Environment 17 got reward of 1.4214231967926025\n",
            "[1] Environment 28 got reward of 1.4234546422958374\n",
            "[2] Environment 19 got reward of 1.424985408782959\n",
            "[2] Environment 28 got reward of 1.424985408782959\n",
            "[5] Environment 8 got reward of 1.4291764497756958\n",
            "[5] Environment 17 got reward of 1.4291764497756958\n",
            "[6] Environments [24] are done\n",
            "[6] Environment 24 got reward of 1.4302071332931519\n",
            "[7] Environment 3 got reward of 1.430940866470337\n",
            "[8] Environment 29 got reward of 1.4315505027770996\n",
            "[8] Environment 30 got reward of 1.4315505027770996\n",
            "[9] Environments [16] are done\n",
            "[10] Environment 9 got reward of 1.4324977397918701\n",
            "[11] Environment 19 got reward of 1.4324533939361572\n",
            "[12] Environments [29] are done\n",
            "[13] Environments [1] are done\n",
            "[14] Environment 28 got reward of 1.4321144819259644\n",
            "[16] Environment 28 got reward of 1.4303642511367798\n",
            "[17] Environment 3 got reward of 1.4284727573394775\n",
            "[18] Environments [30] are done\n",
            "[18] Environment 9 got reward of 1.4259698390960693\n",
            "[18] Environment 15 got reward of 1.4259698390960693\n",
            "[21] Environments [21] are done\n",
            "[24] Environments [13] are done\n",
            "[24] Environment 3 got reward of 1.413061499595642\n",
            "[25] Environment 15 got reward of 1.4102568626403809\n",
            "[27] Environment 5 got reward of 1.4050158262252808\n",
            "[28] Environment 9 got reward of 1.4017540216445923\n",
            "[29] Environment 3 got reward of 1.3969693183898926\n",
            "[29] Environment 15 got reward of 1.3969693183898926\n",
            "[30] Environments [28] are done\n",
            "[31] Environments [20] are done\n",
            "[31] Environment 25 got reward of 1.3892370462417603\n",
            "[34] Environments [19] are done\n",
            "[36] Environment 18 got reward of 1.3756061792373657\n",
            "[36] Environment 25 got reward of 1.3756061792373657\n",
            "[37] Environments [17] are done\n",
            "[38] Environment 15 got reward of 1.3700670003890991\n",
            "[38] Environment 24 got reward of 1.3700670003890991\n",
            "[39] Environments [7, 8] are done\n",
            "[40] Environments [3, 10] are done\n",
            "[43] Environments [26] are done\n",
            "[43] Environment 9 got reward of 1.3611024618148804\n",
            "[43] Environment 26 got reward of 1.3611024618148804\n",
            "[44] Environments [22] are done\n",
            "[45] Environments [23] are done\n",
            "[47] Environment 28 got reward of 1.355597972869873\n",
            "[49] Environment 24 got reward of 1.352995753288269\n",
            "[50] Environment 2 got reward of 1.3516316413879395\n",
            "[52] Environment 31 got reward of 1.3491106033325195\n",
            "[53] Environment 9 got reward of 1.346511960029602\n",
            "[53] Environment 24 got reward of 1.346511960029602\n",
            "[54] Environment 10 got reward of 1.3439948558807373\n",
            "[57] Environment 30 got reward of 1.3368761539459229\n",
            "[57] Environment 31 got reward of 1.3368761539459229\n",
            "[58] Environments [20, 25] are done\n",
            "[59] Environments [5] are done\n",
            "[61] Environments [0, 12] are done\n",
            "[62] Environment 2 got reward of 1.3267364501953125\n",
            "[62] Environment 29 got reward of 1.3267364501953125\n",
            "[64] Environment 15 got reward of 1.3223674297332764\n",
            "[66] Environments [18] are done\n",
            "[70] Environments [6] are done\n",
            "[71] Environment 19 got reward of 1.3079423904418945\n",
            "[71] Environment 31 got reward of 1.3079423904418945\n",
            "[72] Environments [4] are done\n",
            "[74] Environment 30 got reward of 1.3020087480545044\n",
            "[75] Environments [9, 15] are done\n",
            "[75] Environment 28 got reward of 1.299904465675354\n",
            "[76] Environments [31] are done\n",
            "[76] Environment 19 got reward of 1.2997570037841797\n",
            "[77] Environments [23] are done\n",
            "[81] Environment 2 got reward of 1.3016587495803833\n",
            "[82] Environment 27 got reward of 1.3018075227737427\n",
            "[83] Environments [2, 17] are done\n",
            "[83] Environment 11 got reward of 1.3019541501998901\n",
            "[86] Environments [16] are done\n",
            "[90] Environments [3] are done\n",
            "[90] Environment 19 got reward of 1.3065646886825562\n",
            "[91] Environment 5 got reward of 1.3069523572921753\n",
            "[92] Environments [5, 19] are done\n",
            "[92] Environment 0 got reward of 1.3070752620697021\n",
            "[92] Environment 11 got reward of 1.3070752620697021\n",
            "[92] Environment 12 got reward of 1.3070752620697021\n",
            "[93] Environments [14] are done\n",
            "[94] Environments [21] are done\n",
            "[95] Environments [8, 11, 27] are done\n",
            "[95] Environment 27 got reward of 1.3090592622756958\n",
            "[96] Environments [24] are done\n",
            "[97] Environment 29 got reward of 1.3111337423324585\n",
            "[98] Environment 12 got reward of 1.311982274055481\n",
            "[100] Environments [30] are done\n",
            "[100] Environment 12 got reward of 1.3133119344711304\n",
            "[105] Environments [4] are done\n",
            "[105] Environment 12 got reward of 1.315922737121582\n",
            "[107] Environment 0 got reward of 1.3160102367401123\n",
            "[109] Environments [29] are done\n",
            "[111] Environments [22] are done\n",
            "[114] Environment 0 got reward of 1.3168920278549194\n",
            "[117] Environments [13] are done\n",
            "[118] Environments [12, 14] are done\n",
            "[118] Environment 0 got reward of 1.3164323568344116\n",
            "[119] Environment 21 got reward of 1.3167946338653564\n",
            "[123] Environment 8 got reward of 1.318440556526184\n",
            "[125] Environments [7, 23] are done\n",
            "[125] Environment 21 got reward of 1.3191322088241577\n",
            "[127] Environment 1 got reward of 1.3196780681610107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj4uor27N_oo",
        "outputId": "852f284a-b99f-42d2-9fc8-907cc0e328c1"
      },
      "source": [
        "h.num_envs=2\n",
        "model = PPO(print_output=True)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fosaLZ9_JAbj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_policy(model, \n",
        "                    nr_of_levels,\n",
        "                    print_output=True):\n",
        "  \"\"\"\n",
        "  TODO: Add Video generation\n",
        "  \"\"\"\n",
        "  policy = model.policy\n",
        "\n",
        "  #pick levels we did not train on. \n",
        "  eval_env = utils.make_env(model.num_envs, start_level=model.num_levels, num_levels=nr_of_levels)\n",
        "  obs = eval_env.reset()\n",
        "\n",
        "  #book-keeping\n",
        "  completed_envs= []\n",
        "  counter_compl_envs = np.zeros(model.num_envs)\n",
        "  episode_rewards = np.zeros(model.num_envs)  #current episode rewards\n",
        "  rewards = {}\n",
        "  for i in range(model.num_envs):\n",
        "    rewards[i] = []\n",
        "  step_counter = 0\n",
        "\n",
        "  policy.eval()\n",
        "  while True:\n",
        "\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    \n",
        "    #if any reward, update envs still not done\n",
        "    for i in range(len(reward)):\n",
        "      if reward[i] != 0 and i not in completed_envs:\n",
        "        episode_rewards[i] += reward[i]\n",
        "    \n",
        "    # If new environment done, complete it\n",
        "    for i in [index for index in range(len(done)) if done[index] == True]:\n",
        "      if i not in completed_envs:\n",
        "        counter_compl_envs[i] += 1\n",
        "        if print_output:\n",
        "          print(\"Environment {:2d} completed its {:4d}th level at timestep {:6d} with a reward of {:10f}\".format(i, int(counter_compl_envs[i]), step_counter, episode_rewards[i]))\n",
        "        rewards[i].append(episode_rewards[i])\n",
        "        episode_rewards[i] = 0\n",
        "        if counter_compl_envs[i] == nr_of_levels:\n",
        "          completed_envs.append(i)  \n",
        "        \n",
        " \n",
        "\n",
        "    # If all environments are done, break\n",
        "    if len(completed_envs) == model.num_envs:\n",
        "      break\n",
        "    step_counter +=1\n",
        "  # end while\n",
        "  \n",
        "  # Calculate average return\n",
        "  total_reward = []\n",
        "  for key, value in rewards.items():\n",
        "    total_reward.append(sum(value))\n",
        "  total_reward = np.mean(total_reward)/nr_of_levels\n",
        "\n",
        "  if print_output:\n",
        "    print('Average return:', total_reward)\n",
        "\n",
        "  return total_reward, episode_rewards\n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM1n8FMIO5ms",
        "outputId": "1d6a252d-c787-4e6d-ac6e-eb6a8745a9ec"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "total_reward, episode_rewards = evaluate_policy(model, 1)\n",
        "print(time.time()-start)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment  1 completed its    1th level at timestep    135 with a reward of  14.658671\n",
            "Environment  0 completed its    1th level at timestep    136 with a reward of  11.875379\n",
            "Average return: 13.267025083303452\n",
            "0.6281616687774658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7EF_ApcQ-kL",
        "outputId": "b3756473-a800-4bdf-efd8-6baf229928e2"
      },
      "source": [
        "start = time.time()\n",
        "total_reward, episode_rewards = evaluate_policy(model, 2)\n",
        "print(time.time()-start)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment  0 completed its    1th level at timestep     91 with a reward of   1.989494\n",
            "Environment  1 completed its    1th level at timestep    135 with a reward of  17.390955\n",
            "Environment  0 completed its    2th level at timestep    179 with a reward of   1.148283\n",
            "Environment  1 completed its    2th level at timestep    254 with a reward of   2.908285\n",
            "Average return: 5.859254255890846\n",
            "1.1824548244476318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48h4whJwVxfg",
        "outputId": "e5c80107-07e7-4c46-e5ec-81aa34ff702d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start = time.time()\n",
        "total_reward, episode_rewards = evaluate_policy(model, 4)\n",
        "print(time.time()-start)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment  1 completed its    1th level at timestep     80 with a reward of   0.000000\n",
            "Environment  0 completed its    1th level at timestep    113 with a reward of  18.180873\n",
            "Environment  0 completed its    2th level at timestep    186 with a reward of   1.596141\n",
            "Environment  1 completed its    2th level at timestep    248 with a reward of   3.190590\n",
            "Environment  0 completed its    3th level at timestep    281 with a reward of   1.610836\n",
            "Environment  1 completed its    3th level at timestep    385 with a reward of   3.380248\n",
            "Environment  0 completed its    4th level at timestep    433 with a reward of   8.308644\n",
            "Environment  1 completed its    4th level at timestep    459 with a reward of   1.328211\n",
            "Average return: 4.699442803859711\n",
            "2.083005428314209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxyY7edYbqAI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}